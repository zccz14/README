---
"title": "LLM生成コードの可観測性とエンジニアリング手法に関する考察"
"summary": "本稿は、著者とHoboとの、LLM生成コードの本番環境での活用に関する議論を記録したものです。核心的な見解は以下の通りです：LLMが生成したコードは、そのまま本番環境で使用することはできず、厳格なテストと可観測性による保証が必要です。可観測性には、侵入型の計装、リソース分離、アラートシステムが必要であり、アラートルールをコードに直接埋め込むことを提案します。著者とHoboは、LLMの知能レベルとエンジニアリング手法の重要性について意見が分かれました。著者は、現在の段階ではプロンプトチェーンやテストプロセスなどのエンジニアリング手法がより重要であると考えていますが、Hoboはモデルの知能の根本的な役割を強調しており、両者の視点はチームにとって補完的で価値があるものです。"
"tags":
  - "LLM"
  - "可観測性"
  - "コード生成"
  - "エンジニアリング手法"
  - "人工知能"
  - "本番環境"
  - "テスト"
"date": "2026-01-11"
---

現在は2026年1月11日、日曜日、未明です。

昨日、久しぶりにHoboと昼食を共にし、多くのことを話し合いました。彼は私たちの近況や仕事について非常に気にかけてくれ、多くの情報交換ができました。

外資系企業で働く彼が、GPTやClaude OpusなどのLLMを無制限に使用して業務を補助し、効率を上げていることをとても羨ましく思います。それに比べ、国内の仕事環境では、これらのツールの使用にはまだ多くの制限や不便さがあります。

私たちの共通認識は、現在のコーディング作業において、LLMが書いたコードはそのまま本番環境に投入することはできず、非常に信頼性が低いということです。

### 可観測性

私は彼に尋ねました。もし一つのモジュール内に限定し、厳格な単体テストとベンチマークテストを通過した場合、使用できるでしょうか？彼はさらに、長期間のサービス安定性を考慮すると、非常に優れた可観測性も必要だと補足しました。
さらに、巨大なシステムをそのような明確に定義された小さなモジュールに分割すること自体、非常にコストが高いのです。

これは確かに私が以前見落としていた問題です。[この記事](czon://f14dd5dd9a733022055d249db9b1ed3d60d9b60b7eb8c063fe24c02774b6b631) で述べたように、インターフェーススタイルテスト、単体テスト、ベンチマークテストを通過して初めて、人はLLM生成コードを信頼できるようになります。
かつて、CPUとメモリ使用量を考慮したベンチマークテストを構築したことがありますが、通常のベンチマークテストだけでは、LLM生成コードの性能問題を発見することはできませんでした。問題を発見するには、事前に負荷テストを行う必要があります。
負荷テストもまた、本番環境の様々な複雑なシナリオを正確にシミュレートすることはできません。したがって、最終的には非常に優れた可観測性がなければ、本番環境でLLM生成コードを使用することはできません。

しかし、可観測性はどのように設計し、テストすべきでしょうか？

可観測性自体も、実際の状況が期待通りであるかをテストするツールですが、その実行環境はテスト環境ではなく、本番環境です。

さらに、十分な情報を収集するためには、実装コードに侵入する必要があるかもしれません。（侵入型の計装は通常、より高い保守コストを意味します）

インターフェースの外部や環境情報から単純にいくつかの指標データを収集するだけでは、問題の一部しか発見できないことがよくあります。
例えば、モジュール内部の状態が正しいか、過剰なリソースを占有していないか、メモリリークがないか、デッドロックがないかなどを観測することはできません。

また、可観測性の指標は、CPU、メモリ、I/Oなど、リソース分離と関係していることが多いです。非常に優れたリソース分離がなければ、問題を発見するのは難しいことがよくあります。

そして、可観測性の鍵はアラートシステムです。Hoboはかつて、「すべての計装ポイントは、対応するアラートルールを持つべきであることを暗示しており、そうでなければその計装ポイントは意味がない」と述べていました。

通常の実践では、アラートルールは運用作業ですが、計装は開発作業です。おそらく、この実践自体に問題があるのでしょう。なぜコードに直接アラートルールを埋め込むことを考えないのでしょうか？

例えば、各計装ポイントがアラートルールの定義を保持し、指標がある閾値を超えたときに自動的にアラートをトリガーするようにできます。これにより、開発者はコードを書く際に、直接可観測性とアラートルールを考慮に入れることができ、コードの品質と信頼性を向上させることができます。

例えば、計装と同時に、アサーション機構を設計することもできます。アサーションが通らなければ、アラートをトリガーします。これはエラー/警告機構に似ているように思えます。エラー/警告ログを記録することは、注目される必要があることを意味するのでしょうか？

まずはログから始めて、エラー/警告ログを重点的に記録し、これらのログを可観測性の一部として、アラートシステムと組み合わせて、システムの信頼性を向上させることができます。

私はHoboの見解に非常に同意します：本番環境に投入するコードは、非常に優れた可観測性を持っていなければならず、そうでなければ長期間の安定性を保証することはできません。

### LLMの知能レベル vs エンジニアリング手法

また、Hoboは、LLM自身の知能レベルがコード品質に与える影響についても言及しました。ここで私たちの意見は分かれました。

彼は、LLMの知能レベルがコード品質を決定する重要な要素であり、知能レベルが不十分であればタスクを完了できないと考えています。
一方、私は、LLMの知能レベルも重要ですが、より重要なのは、タスクとテストプロセスを適切に設計し、生成されるコードが期待通りであることを確実にすることだと考えています。

Hoboはエリートの能力、つまり**天賦主義**の視点に傾いています。一方、私はシステム最適化、つまり**構築主義**の視点に傾いています。

どちらも正しいですが、段階が異なります。

- モデルの能力の臨界点以下の場合、私は絶対に正しいです。現在の大多数の商業アプリケーションにとって、エンジニアリング手法の価値は、次の「より賢い」モデルを待つことよりもはるかに大きいです。注意深く設計されたプロンプトチェーン、完全なテストスイート、反復プロセスにより、能力が中程度のモデルでも安定した使用可能なコードを生成することが完全に可能です。これは現在、AIアプリケーションを実用化する主流であり、成功への道です。

- 真の認知的限界に直面した場合、Hoboの視点が現れます。タスクの複雑さが、真の理解、抽象化、革新を必要とするレベルに達したとき（例えば、全く新しいアルゴリズムを設計する、または非常に曖昧で矛盾する要求を理解する）、モデルの「知能の天井」は乗り越えられない障害となります。このとき、どんなに優れたプロセスでも、モデルが「認知的にできない」ことを完了させることはできません。

私が代表するのは「エンジニアの現実主義」であり、現在AIに価値を創造させる核心的な推進力です。
Hoboが代表するのは「研究者の先見性」であり、将来の能力の突破口に注目しています。

最も理想的な状態は、「エリートレベルの知能」に「エリートレベルのエンジニアリング手法」を組み合わせることです。

最良のプロセスを用いて、最も強力な知能を引き出し、制御することです。私たちの意見の相違は、正誤の問題ではなく、注目点（現在の最適化 vs 根本的な突破）と時間尺度（短期間での実用化 vs 長期的な進化）の違いです。

チームにおいて、このような補完的な視点は非常に貴重です。