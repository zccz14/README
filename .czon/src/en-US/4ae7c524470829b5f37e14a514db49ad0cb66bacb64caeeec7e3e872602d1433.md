---
"title": "Reflections on AI Development Experience: Limitations and Improvement Directions of LLMs from Building CZONE"
"summary": "This article documents the author's experience on January 19, 2026, using OpenCode and MiniMax M2.1 to build the online version of CZON (CZONE) from scratch. AI was fast in technology selection, scaffolding setup, and feature design, but showed insufficient attention to detail when handling GitHub REST API permission issues, particularly failing to recognize the special permission requirements for the `.github/workflows` directory. The author points out that LLMs suffer from attention dispersion and weak reasoning in debugging mode, suggesting the introduction of a 'Lab Mode' for controlled experiment validation. Meanwhile, OpenCode lacks browser control capabilities, forcing debugging to rely on manual log inspection; the author recommends integrating end-to-end testing frameworks like Cypress or Playwright. Furthermore, the AI development pace was too rapid, lacking architectural layering and quality assurance, which the author metaphorically describes as 'floodwater,' emphasizing the need for correct concepts, abstractions, and implementations. The article concludes with a plumber-fixing-a-leak analogy, implying that AI development requires systematic solutions to root problems rather than temporary patches."
"tags":
  - "AI Development"
  - "LLM Limitations"
  - "OpenCode"
  - "Debugging Capability"
  - "GitHub API"
  - "Permission Management"
  - "Development Pace"
  - "Lab Mode"
"date": "2026-01-19"
---

It is the afternoon of Monday, January 19, 2026.

I got up very late today because I was quite excited last night tinkering with CZONE and OpenCode. Although the results weren't satisfactory, if I hadn't tried, yesterday's outcome might have been even more of a failure.

> Staying up late is just refusing to admit the failure of one's day.
> — from PH

Last night, I used OpenCode + MiniMax M2.1 to build CZONE (the online version of CZON) from scratch, as seen in [this log](czon://ec05a61746e4d7898feb28f18729f0ab3df43b67569d88f217ae6472505c5f66).

The AI started asking a bunch of questions, from technology selection to scaffolding setup, then to feature design, and finally to the CI/CD process. The whole thing was very fast.

Honestly, a bit too fast—I felt a bit carsick (laughs).

But, the crucial *but*, problems arose quickly.

I found that its understanding of the details was inadequate when solving GitHub REST API permission issues.

**Vast Knowledge? Not Really.**

For example, after initializing the repo, we needed to modify the `.github/workflows/pages.yml` file to add the CZON build steps. This requires `workflow` scope permissions, but the code provided by OpenCode did not include this permission. A quick glance at the GitHub API documentation would reveal this. Yet, it repeatedly overlooked this detail. Also, GitHub is dumb; the error message was just a 404 with no hint of insufficient permissions. It completely failed to recognize this issue.

During this process, we demonstrated that writing to `index.md` succeeded, writing to `.github/index.md` succeeded, writing to `github/workflows/pages.yml` also succeeded—only `.github/workflows/pages.yml` failed. Although the conversation went through multiple rounds because it kept tweaking the code each time, given how obvious it was, it didn't realize that `.github/workflows/` might be a directory requiring special permissions. This indicates its attention is scattered, and its reasoning ability in debugging mode is not strong enough.

I strongly suggest that LLMs themselves or external control framework Agents need a **Lab Mode**. In this mode, the Agent should repeatedly design controlled experiments, verify results, and thus find the truth. I sometimes feel an LLM is like a brain without consciousness—you poke somewhere, and it lights up there. It focuses on whatever the prompt says.

Sometimes we want it to be vastly knowledgeable, other times we wish it were ignorant to the point of clarity. In a sense, the energy consumed by an LLM is constant; we hope it can allocate that energy to where it's most needed for different tasks, rather than distributing it evenly. Recent advancements in the LLM field often adopt this approach.

**A Brain in a Vat, Limited in Agency**

Another important and annoying reason is OpenCode's insufficient self-debugging capability. It completely lacks the ability to open and control a browser, so it can only frequently guess, log, and then ask me to check the logs. Sometimes I play along, but other times, watching it is like watching my mentee—I have no idea what it's thinking, just feeling anxious. I can accept having a笨笨的 mentee, but I probably can't accept it being a 'brain in a vat without hands'—we still need to find a way to close the loop of its thinking. Google's Antigravity next door does a good job, probably also due to the Chrome family connection.

In terms of community solutions, using some end-to-end testing frameworks (e.g., Cypress, Playwright) to control the browser should be a good choice. After all, many operations nowadays require browser-side interaction; relying solely on APIs is not enough.

**Progress Too Fast, Foundation Unstable**

The last point is also my attribution. This time, the AI started from zero and wrote dozens of files for me in less than 10 minutes. Watching it was like watching a printer—it never seemed to pause for a break. However, any complex system requires architecture, layering, and ensuring the basic quality of each module. Only after completing the底层 and implementing tests can you confidently build the上层. AI currently has no sense of this rhythm; it just prints code. While having built-in debugging capabilities might allow it to flexibly modify up and down, what truly prevents problems fundamentally relies on correct concepts, correct abstractions, correct implementations—on things making logical sense, on coherence. As for how much time AI will spend on this, I think it's still far from adequate. Perhaps this is something only a coordination layer can solve; LLMs alone can't do it. **LLMs只管开路, like floodwater, pouring into the lowest point of potential energy.**

![Debugging is like..](https://media.tenor.com/97gs87bOyQAAAAAM/debugging-programming.gif)

But many humans are like this too. There's a classic old joke: A plumber fixes a leak, patching one spot only for another to burst. Treating the head when the head aches, treating the foot when the foot hurts. In the end, the root problem can't be solved, and you just end up rowing a boat in the water.