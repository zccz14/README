---
"title": "Reflections on AI Development Experience: Limitations and Improvement Directions of LLMs from Building CZONE"
"summary": "This article documents the author's experience on January 19, 2026, using OpenCode and MiniMax M2.1 to build the online version of CZON (CZONE) from scratch. AI was fast in technology selection, scaffolding setup, and feature design, but showed insufficient attention to detail when handling GitHub REST API permission issues, particularly failing to recognize the special permission requirements for the `.github/workflows` directory. The author points out that LLMs suffer from attention dispersion and weak reasoning in debugging mode, suggesting the introduction of a 'Lab Mode' for controlled experimental validation. Simultaneously, OpenCode lacks browser control capabilities, forcing debugging to rely on manual log inspection; the author recommends integrating end-to-end testing frameworks like Cypress or Playwright. Furthermore, the AI's development pace was too rapid, lacking architectural layering and quality assurance, which the author metaphorically describes as 'floodwater released from a dam,' emphasizing the need for correct concepts, abstractions, and implementations. The article concludes with a plumber joke about fixing leaks, implying that AI development requires systematic solutions to root problems rather than temporary patches."
"tags":
  - "AI Development"
  - "LLM Limitations"
  - "OpenCode"
  - "Debugging Capability"
  - "GitHub API"
  - "Permission Management"
  - "Development Pace"
  - "Lab Mode"
"date": "2026-01-19"
---

It is now the afternoon of Monday, January 19, 2026.

I got up very late today because I was quite excited last night tinkering with CZONE and OpenCode. Although the results weren't satisfactory, if I hadn't experimented, yesterday's outcome might have been even more of a failure.

> Staying up late is just refusing to admit the failure of one's day.
> — from PH

Last night, I used OpenCode + MiniMax M2.1 to build CZONE (the online version of CZON) from scratch, as detailed in [this log](./18.md).

The AI started by asking a flurry of questions—from technology selection to scaffolding setup, then to feature design, and finally to the CI/CD pipeline. The whole process was incredibly fast.

Honestly, it was a bit too fast; I felt a bit motion-sick (laughs).

But, the crucial *but*, problems arose quickly.

I found that its understanding of the details was lacking when solving GitHub REST API permission issues.

**Extensive knowledge? Not really.**

For example, after initializing the repo, we needed to modify the `.github/workflows/pages.yml` file to add the CZON build steps. This requires the `workflow` scope permission, but the code provided by OpenCode did not include this permission. A quick glance at the GitHub API documentation would reveal this. Yet, it repeatedly overlooked this detail. Also, GitHub is rather silly—the error message was just a 404, with no hint of insufficient permissions. It completely failed to recognize this issue.

During this process, we demonstrated that writing to `index.md` succeeded, writing to `.github/index.md` succeeded, and writing to `github/workflows/pages.yml` also succeeded. Only `.github/workflows/pages.yml` failed. Although the conversation went through multiple rounds because it kept modifying the code each time, given how obvious this was, it didn't realize that `.github/workflows/` might be a directory with special permission requirements. This indicates its attention is scattered, and its reasoning ability in debugging mode is not strong enough.

I strongly suggest that LLMs themselves or external control framework Agents need a **Lab Mode**. In this mode, the Agent should repeatedly design controlled experiments, verify results, and thereby uncover the truth. I sometimes feel an LLM is like an unconscious brain—you poke somewhere, and it lights up there. Whatever the prompt says, it focuses on that.

Sometimes we want it to be extensively knowledgeable, and other times we wish it were ignorant to the point of clarity. In a sense, the energy consumed by an LLM is constant. We hope it can allocate that energy to the most needed areas for different tasks, rather than distributing it evenly. Recent advancements in the LLM field often adopt this approach.

**A Brain in a Vat, Limited in Agency**

Another important and annoying reason is OpenCode's insufficient self-debugging capability. It completely lacks the ability to open and control a browser, so it can only frequently guess, log outputs, and then ask me to check the logs. I sometimes play along with it, but other times, watching it is like watching my mentee—I have no idea what it's thinking, just feeling utterly helpless. I can accept having a somewhat笨笨的 mentee, but I probably can't accept it being a 'brain in a vat without hands'—we still need to find ways to close the loop of its thinking. Google's Antigravity next door does a pretty good job, probably also due to the Chrome family connection.

In terms of community solutions, using some end-to-end testing frameworks (e.g., Cypress, Playwright) to control the browser should be a good choice. After all, many operations nowadays require browser-side interaction; relying solely on APIs is not enough.

**Progress Too Fast, Foundation Unstable**

The last point is also my attribution. This time, the AI, starting from zero, wrote dozens of files for me in less than 10 minutes. Watching it was like watching a printer—it had no sense of pausing to rest. However, any complex system requires architecture, layering, and ensuring the basic quality of each module. Only after completing the底层 and implementing tests can you confidently work on the上层. AI currently has no sense of this rhythm; it just prints code. While having built-in debugging capabilities might allow it to flexibly modify up and down the stack, truly avoiding problems fundamentally relies on correct concepts, correct abstractions, correct implementations—it depends on logical coherence, on making sense. As for how much time AI will spend on this, I think it's still far from adequate. Perhaps this is something only a coordination layer can solve; LLMs alone cannot achieve it. **LLMs只管开路, like floodwater released from a dam,倾泻到势能的最低处.**

![Debugging is like..](https://media.tenor.com/97gs87bOyQAAAAAM/debugging-programming.gif)

But many humans are like this too. There's a classic joke: A plumber fixes a leak, patching one spot only for another to burst open. Treating the head when the head aches, treating the foot when the foot hurts. In the end, the根本 problem can't be solved, and you're left rowing a boat in the water.