---
"title": "Observability and Engineering Approaches for LLM-Generated Code"
"summary": "This article documents the author's discussion with Hobo regarding the application of LLM-generated code in production environments. Key points include: LLM-generated code cannot be used directly in production and must be safeguarded by rigorous testing and observability; observability requires intrusive instrumentation, resource isolation, and alerting systems, with a suggestion to embed alert rules within the code; the author and Hobo have differing views on the importance of LLM intelligence versus engineering methods. The author believes engineering methods (such as prompt chains, testing processes) are more critical at the current stage, while Hobo emphasizes the fundamental role of model intelligence. Both perspectives are complementary and valuable to a team."
"tags":
  - "LLM"
  - "Observability"
  - "Code Generation"
  - "Engineering Methods"
  - "Artificial Intelligence"
  - "Production Environment"
  - "Testing"
"date": "2026-01-11"
---

It is now early morning, Sunday, January 11, 2026.

Yesterday, I had lunch with Hobo. We hadn't seen each other for a long time, and we talked a lot over the meal. He was very concerned about our recent situation and work. We exchanged many thoughts.

I envy that he, working in a foreign company, can use LLMs like GPT and Claude Opus without limits to assist with work and improve efficiency. In contrast, in our domestic work environment, there are still many restrictions and inconveniences in using these tools.

Our consensus is that in current coding work, code written by LLMs cannot go directly into a production environment; it is very, very unreliable.

### Observability

I asked him, if confined to a single module and having passed rigorous unit tests and benchmark tests, could it be used? He added that excellent observability is also needed, as long-term service stability must be considered.
Furthermore, the cost of decomposing a large system into many such well-defined small modules is itself very high.

This is indeed a point I had previously overlooked. In [this article](../INSIGHTS/1.md), I mentioned that after passing interface style tests, unit tests, and benchmark tests, one can begin to trust LLM-generated code.
I once constructed a benchmark test that considered CPU and memory usage. Relying solely on ordinary benchmark tests, it was impossible to detect performance issues in LLM-generated code. You had to apply stress tests in advance to uncover problems.
Stress tests also cannot truly simulate the various complex scenarios in a production environment. Therefore, ultimately, excellent observability is required to use LLM-generated code in production.

But how should observability be designed and tested?

Observability itself is a tool for testing whether actual conditions meet expectations, but it operates in the production environment, not the testing environment.

Moreover, it may need to intrude into the implementation code to collect sufficient information. (Intrusive instrumentation typically implies higher maintenance costs.)

If we simply collect some metric data from outside the interface and environmental information, we often only discover a portion of the problems.
For example, we cannot observe whether the internal state of the module is correct, whether it is consuming excessive resources, whether there are memory leaks, deadlocks, etc.

Also, observability metrics are often related to resource isolation, such as CPU, memory, I/O, etc. Without excellent resource isolation, it is often difficult to detect issues.

Furthermore, the key to observability lies in the alerting system. Hobo once mentioned, "Every instrumentation metric implies it should have a corresponding alert rule; otherwise, the instrumentation point is meaningless."

In common practice, alert rules are an operational task, while instrumentation is a development task. Perhaps this practice itself is problematic. Why don't we consider embedding alert rules directly in the code?

For instance, each instrumentation point could carry a definition of an alert rule. When a metric exceeds a certain threshold, an alert is automatically triggered. This way, developers can directly consider observability and alert rules while writing code, thereby improving code quality and reliability.

For example, along with instrumentation, an assertion mechanism could be designed. If an assertion fails, it triggers an alert. This seems similar to an error/warning mechanism. Does logging an error/warning imply it needs attention?

We could start with logs, focusing on recording error/warning logs, treating these logs as part of observability, and combining them with the alerting system to enhance system reliability.

I strongly agree with Hobo's point: Code deployed to production must have excellent observability; otherwise, long-term stability cannot be guaranteed.

### LLM Intelligence Level vs. Engineering Methods

Additionally, Hobo raised the issue of how the LLM's own intelligence level affects coding quality. Here we have some disagreement.

He believes that the LLM's intelligence level is the key factor determining coding quality. Insufficient intelligence means the task cannot be completed.
I believe that while the LLM's intelligence level is important, what's more critical is how to design tasks and testing processes well to ensure the generated code meets expectations.

Hobo leans towards elite capability, a **meritocratic** viewpoint; whereas I lean more towards system optimization, a **constructivist** viewpoint.

Both are correct, but at different stages.

-   **Below the model capability threshold, I am absolutely correct.** For the vast majority of current commercial applications, the value of engineering methods far outweighs waiting for the next "smarter" model. A well-designed prompt chain, a comprehensive test suite, and an iterative process can completely enable a moderately capable model to produce stable, usable code. This is the mainstream and successful path for current AI application deployment.

-   **When facing true cognitive limits, Hobo's viewpoint becomes apparent.** When task complexity reaches a level requiring genuine understanding, abstraction, and innovation (e.g., designing a brand-new algorithm or understanding an extremely vague, contradictory requirement), the model's "intelligence ceiling" becomes an insurmountable obstacle. At this point, no process, no matter how good, can make the model accomplish something it is "cognitively incapable of."

I represent the **"pragmatism of the engineer,"** the core driving force for creating value with AI in the present.
Hobo represents the **"foresight of the researcher,"** focusing on the breakthrough points for future capabilities.

The ideal state is **"elite-level intelligence" combined with "elite-level engineering methods."**

Using the best processes to stimulate and harness the most powerful intelligence. Our disagreement is not about right or wrong, but about focus points (current optimization vs. fundamental breakthroughs) and time scales (short-term deployment vs. long-term evolution).

Within a team, this complementary perspective is extremely valuable.