---
title: Module-Level Human-AI Collaborative Software Engineering Architecture
summary: This paper explores the limitations of LLMs in module-level human-AI collaborative software engineering, such as non-mandatory coordination and limited computational power. It proposes methods to reduce human intervention through interface naming, unit testing, and benchmarking, aiming to improve efficiency and reduce costs.
tags:
  - Human-AI Collaboration
  - Software Engineering
  - LLM
  - AI Agent
  - Module Design
  - Human Intervention
  - Efficiency Optimization
inferred_lang: en-US
---

# Software Engineering Architecture for Module-Level Human-AI Collaboration

## Problem Background

Design an LLM-based engineering architecture for module-level human-AI collaboration, aiming to efficiently complete the design, implementation, and iteration of industrial-grade application modules while reducing the cost of human intervention.

1.  Existing AI Agents (Claude Code, CodeX) produce poor-quality code for module implementation, still requiring significant human intervention, rework, and review.
2.  Existing AI Agents struggle to define module boundaries during implementation, leading to the creation of code with unnecessary complexity.
3.  Existing AI Agents are too slow; a task from assignment to acceptance takes 10-30 minutes.

## Problem Insights

### Why Can't Agents Complete All Work in One Go?

Because LLM intelligence is inherently limited.

There are two main reasons: (Architectural Constraint) Non-mandatory coordination and (Resource Constraint) Limited computational budget. A secondary reason is (Cognitive Constraint) Cognitive incompressibility.

**Non-mandatory coordination**: LLMs are forced to say or do *something*, but there is no guarantee the result will adhere to all constraints. If you assume an LLM must answer while also forcing it to obey all constraints, then when you pose a difficult or even unsolvable problem, the computation would become paralyzed. At their core, LLMs are designed with non-mandatory coordination, opting instead to output the token with the highest probability. (Halting Problem)
Some difficult problems require finding a good solution among complex, competing concerns. An LLM's context cannot simultaneously encompass project management, design, implementation, and acceptance—these are completely conflicting and adversarial. Only an architecture with separation of concerns can prevent the Agent from getting stuck. Before outputting, the LLM is not actually forced to comply with all constraints, as that would cause a breakdown in reasoning.

**Limited computational budget**: Clearly, a smarter LLM would have the capacity to handle more concerns, but the cost would inevitably be higher computational power. The so-called AGI is always the ceiling goal under conditions of unlimited resources, not the goal of a commercially viable, general-purpose LLM. Only the business can decide whether to use such high computational power to solve a problem; therefore, a general-purpose LLM cannot make this decision on its own. This fundamental contradiction cannot be resolved. (Economic Constraint)
Note the Münchhausen trilemma: the infinity of thought itself is in perpetual conflict with the finiteness of the thought budget. Ideally, we hope AI becomes a god-like existence, but in reality, it can only ever be limited intelligence, precisely because its thinking consumes resources.

A relatively secondary reason:

**Cognitive incompressibility**: Business logic requires sufficient information input to be solved. If we think a single sentence can make it clear to an LLM, it means this cognitive transmission is compressible. In reality, you need to supplement a lot of context for the LLM to understand the complexity of the environment.
Why is this a secondary reason? This challenge has some nuances: common sense is already embodied in the LLM's pre-training, and many cognitive issues will gradually become minor as LLMs grow more powerful. Secondly, LLMs can first guess and complete, then quickly align with humans, which also mitigates the contradiction. However, based on the current state, it still poses a significant problem.
Many experienced LLM users can recognize the issues this brings.

However, good news is that the threshold for Agent emergence is that the Agent can at least produce effective output for one concern, which has clearly been unlocked. We need to long-term leverage this limited intelligence as a tool to accomplish our business.

### How to Reduce Human Intervention?

The key is to alleviate human concerns. Then, people, adopting a mindset of "it's usable enough," will stop further demanding more from the AI's work output.

So, after which checks will a person judge that they no longer have the ability to intervene or that it's unnecessary to take further action?

1.  The conceptual naming of the module's external interfaces aligns with taste and requirements. This alleviates the concern that unreasonable interfaces will proliferate downstream in the system.
2.  It passes unit tests. This alleviates the concern about whether the module functions correctly.
3.  It shows optimization or no degradation in benchmark tests. This alleviates the concern about whether the module is inefficient.
    The first point can be identified in the initial stage, while the latter two are only known after the experiment concludes. If all three are satisfied, humans have little reason to forcibly intervene in the AI's completed work.

As for whether this module can truly handle real data patterns, it needs to be tested with production environment data. Then, humans summarize its patterns and construct a new module through intent to solve new problems. This issue is temporarily outside the scope of this paper.

### Priority Goals

1.  Reduce human intervention.
2.  Reduce runtime, improve speed.
3.  Reduce Token usage, lower LLM costs.