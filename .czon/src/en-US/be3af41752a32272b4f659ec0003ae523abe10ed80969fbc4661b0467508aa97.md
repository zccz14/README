---
"title": "Observability and Engineering Approaches for LLM-Generated Code"
"summary": "This article documents the author's discussion with Hobo regarding the application of LLM-generated code in production environments. Key points include: LLM-generated code cannot be used directly in production and must be ensured through rigorous testing and observability; observability requires intrusive instrumentation, resource isolation, and alerting systems, with suggestions to embed alert rules within the code; the author and Hobo differ on the importance of LLM intelligence versus engineering methods—the author believes engineering methods (such as prompt chains, testing processes) are more critical at the current stage, while Hobo emphasizes the fundamental role of model intelligence. Both perspectives are complementary and valuable to a team."
"tags":
  - "LLM"
  - "Observability"
  - "Code Generation"
  - "Engineering Methods"
  - "Artificial Intelligence"
  - "Production Environment"
  - "Testing"
"date": "2026-01-11"
---

It is now early morning on Sunday, January 11, 2026.

Yesterday, I had lunch with Hobo. We hadn't seen each other for a long time, so we talked a lot over the meal. He was very concerned about our recent situation and work, and we exchanged many thoughts.

I envy that he, working in a foreign company, can use LLMs like GPT and Claude Opus without limits to assist with work and improve efficiency. In contrast, in our domestic work environment, there are still many restrictions and inconveniences in using these tools.

Our consensus is that in current coding work, code written by LLMs cannot go directly into production—it is very, very unreliable.

### Observability

I asked him, if we limit it to a single module and it passes strict unit tests and benchmark tests, can it be used? He added that excellent observability is also needed, as long-term service stability must be considered.
Moreover, the cost of splitting a large system into many such well-defined small modules is itself very high.

This is indeed a point I had previously overlooked. In [this article](czon://f14dd5dd9a733022055d249db9b1ed3d60d9b60b7eb8c063fe24c02774b6b631), I mentioned that after passing interface style tests, unit tests, and benchmark tests, one can trust LLM-generated code.
I once constructed a benchmark test where, considering CPU and memory usage, ordinary benchmark tests alone could not detect performance issues in LLM-generated code. You must conduct stress tests in advance to uncover problems.
Stress tests also cannot truly simulate the various complex scenarios in a production environment, so ultimately, excellent observability is required to use LLM-generated code in production.

But how should observability be designed and tested?

Observability itself is a tool for testing whether actual conditions meet expectations, but it operates in the production environment rather than the testing environment.

Furthermore, it may need to intrude into the implementation code to collect sufficient information. (Intrusive instrumentation typically implies higher maintenance costs.)

If we simply collect some metric data from outside the interface and environmental information, we can often only detect a portion of the problems.
For example, we cannot observe whether the internal state of the module is correct, whether it is consuming excessive resources, whether there are memory leaks, deadlocks, etc.

Moreover, observability metrics are often related to resource isolation, such as CPU, memory, I/O, etc. Without excellent resource isolation, it is often difficult to identify issues.

Additionally, the key to observability lies in the alerting system. Hobo once mentioned, "Every instrumentation metric implies it should have a corresponding alert rule; otherwise, the instrumentation is meaningless."

In practice, alert rules are typically an operations task, while instrumentation is a development task. Perhaps this practice itself is problematic. Why don't we consider embedding alert rules directly into the code?

For example, each instrumentation point could carry a definition of an alert rule, automatically triggering an alert when a metric exceeds a certain threshold. This way, developers can directly consider observability and alert rules while writing code, thereby improving code quality and reliability.

For instance, while instrumenting, we could also design an assertion mechanism—if an assertion fails, an alert is triggered. This seems similar to an error/warning mechanism. Does logging an error/warning imply it needs attention?

We could start with logs, focusing on recording error/warning logs, treating these logs as part of observability, and combining them with the alerting system to enhance system reliability.

I strongly agree with Hobo's point: code deployed to production must have excellent observability; otherwise, long-term stability cannot be guaranteed.

### LLM Intelligence Level vs. Engineering Methods

Additionally, Hobo mentioned the impact of the LLM's own intelligence level on coding quality. Here, we have some differences of opinion.

He believes that the LLM's intelligence level is the key factor determining coding quality—if the intelligence is insufficient, the task cannot be completed.
I believe that while the LLM's intelligence level is important, what is more critical is how to design tasks and testing processes well to ensure the generated code meets expectations.

Hobo leans toward elite capability, a **talent-centric** view; whereas I lean more toward system optimization, a **constructivist** view.

Both are correct, but at different stages.

- Below the model capability threshold, I am absolutely correct. For the vast majority of commercial applications today, the value of engineering methods far outweighs waiting for the next "smarter" model. A well-designed prompt chain, a comprehensive test suite, and an iterative process can fully enable a moderately capable model to produce stable, usable code. This is the mainstream and successful path for current AI application deployment.

- When facing true cognitive limits, Hobo's view becomes apparent. When task complexity reaches a level requiring genuine understanding, abstraction, and innovation (e.g., designing a brand-new algorithm or understanding extremely vague, contradictory requirements), the model's "intelligence ceiling" becomes an insurmountable obstacle. At this point, no process can make the model accomplish what it "cognitively cannot do."

I represent the "pragmatism of the engineer," the core driver for creating value with AI today.
Hobo represents the "foresight of the researcher," focusing on future breakthroughs in capability.

The ideal state is "elite-level intelligence" plus "elite-level engineering methods."

Using the best processes to stimulate and harness the most powerful intelligence. Our disagreement is not about right or wrong but about focus (current optimization vs. fundamental breakthroughs) and time scale (short-term deployment vs. long-term evolution).

In a team, such complementary perspectives are extremely valuable.