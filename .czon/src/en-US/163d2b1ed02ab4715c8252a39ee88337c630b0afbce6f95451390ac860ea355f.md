---
"title": "Analysis of Agent Performance in Translation Tasks and Improvement Proposals"
"summary": "This article explores why Agents underperform compared to one-shot LLMs in translation tasks, noting that Agents excel in multi-step reasoning and decision-making tasks rather than simple single-step operations. The author found that Agents use 10 times more tokens than LLMs without improving translation quality, and may even enter infinite loops in low-resource language translation. This is attributed to the Agent's context management strategy, which prevents it from fully utilizing contextual information. The author proposes two improvement approaches: using an Agents/Sub-Agents framework to decompose the translation task, or assembling a low-level one-shot LLM API via Skills. The author prefers the first approach for its higher potential ceiling but expresses concerns about OpenCode's support for complex Agent calls. The article also documents CZON's update log, including OpenCode integration, network issue fixes, and slug handling improvements."
"tags":
  - "Agent"
  - "Translation Task"
  - "LLM"
  - "Context Management"
  - "Improvement Proposal"
  - "CZON"
  - "OpenCode"
"date": "2026-01-23"
---

It is now January 23, 2026, early morning.

Unfortunately, I've discovered that Agents perform worse than one-shot LLMs in direct translation tasks. It seems the strength of Agents lies more in tasks requiring multi-step reasoning and decision-making, rather than simple single-step operations. Agents use 10 times the number of tokens as LLMs, yet there is no improvement in translation quality. I initially intended to use them to solve the issue of one-shot LLMs exceeding maximum output length limits in long-text translation tasks, but it appears it's not that straightforward.

I believe this may be because Agent scenarios typically involve complex tasks with massive amounts of information. Therefore, they are designed to read and write to the context as little as possible to increase their processing capacity. This context management strategy means they don't default to incorporating all information, preventing them from leveraging context as effectively as one-shot LLMs for translation.

Interestingly, for some low-resource language translation tasks (e.g., Simplified Chinese to Indonesian), Agents can occasionally exhibit a repetitive translation loop, failing to converge to a stable translated result. This might be due to a flaw in how the Edit tool handles certain languages, causing it to incorrectly replace text and fall into an infinite loop. (Being too frugal can also be problematic.)

I've thought of two potential solutions:

1.  Use an Agents / Sub-Agents framework to decompose the translation task, such as a translation-proofreading adversarial generation framework.
2.  Use Skills to assemble a low-level one-shot LLM API, allowing the Agent to call a translation skill instead of attempting the translation itself.

I might prefer the first solution, as its potential ceiling seems higher. I'm just unsure about OpenCode's level of support for complex Agent calls.

Additionally, here is the CZON update log:

-   0.5.0: Integrated OpenCode for translation tasks. (However, this introduced performance issues when translating large numbers of files, to be fixed later.)
-   0.5.1: Fixed an issue where frontend static resources (tailwindcss) failed to load due to inability to access jsdelivr from within China. (Achieved by downloading CDN files locally during the build phase.)
-   0.5.2: When a slug already exists, it is no longer updated to prevent slug changes when content is modified. (This avoids historical link breakage caused by log file renaming.)