---
"title": "Module-Level Human-AI Collaborative Software Engineering Architecture Design"
"summary": "This paper addresses the issues of poor quality, unclear boundaries, and slow speed in existing AI Agents for code module implementation by proposing a module-level human-AI collaborative software engineering architecture. The architecture generates a Protocol Spec through rapid intention alignment, then parallelly generates implementation, test, and benchmark specifications, ensuring implementation quality through a multi-level arbitration mechanism. The core design includes layered collaboration, specialized division of labor, and separation of concerns, with clear acceptance criteria (unit test passing, performance not degrading) to establish a trust mechanism and eliminate human desire for control. The article also discusses unresolved issues such as improving Protocol Spec quality and avoiding arbitration loops, and envisions the possibility of replacing human supervision with higher-level AI."
"tags":
  - "Human-AI Collaboration"
  - "Software Engineering"
  - "LLM"
  - "AI Agent"
  - "Module Design"
  - "Arbitration Mechanism"
  - "Automated Development"
"date": "2026-01-05"
---

# Module-Level Human-AI Collaborative Software Engineering Architecture

2026-01-05

## Problem Background

Design an LLM-based module-level human-AI collaborative engineering architecture aimed at efficiently completing the design, implementation, and iteration of industrial-grade application modules, reducing the cost of human intervention.

1.  Existing AI Agents (Claude Code, CodeX) produce poor-quality code module implementations, still requiring significant human intervention, rework, and review.
2.  Existing AI Agents struggle to define module boundaries during implementation, resulting in code with unnecessary complexity.
3.  Existing AI Agents are too slow; a task from assignment to acceptance takes 10-30 minutes.

## Problem Insights

-   According to [this article](czon://3a40b7b252512466f0b0feb8c7ea77fc624bb0026cbae8882c5c205462e0aa64), human desire for control stems from rational concerns about losing control over outcomes. Establishing a controllable trust mechanism is the solution.
-   According to [this article](czon://038e3e093437a25f9eb22aabb4debaa185632c4e19a53b3103b614e016160b80), I believe the physical and economic mechanisms of LLMs inherently make it difficult for them to complete all work in one go.

The key to liberating human productivity lies in eliminating the human desire for control over details. Then, humans, adopting a "good enough to use" mentality, will no longer excessively demand perfection from AI's output.

So, what checks, when passed, would lead a human to judge that they are no longer capable of intervening or that further action is unnecessary?

1.  The conceptual naming and taste of the module's external interfaces align with requirements. This alleviates concerns about unreasonable interfaces propagating downstream in the system.
2.  Unit tests pass. This alleviates concerns about whether the module functions correctly.
3.  Benchmark tests show optimization or no degradation. This alleviates concerns about the module's efficiency.
    The first point can be identified in the initial stage, while the latter two are only known after the experiment concludes. If all three are satisfied, humans have little reason to forcibly intervene in the AI's completed work.

As for whether the module can truly handle real-world data patterns, production environment data should be used for testing. Then, humans can summarize the patterns and construct a new module through intention to solve new problems. This issue is temporarily outside the scope of this article.

### Priority Goals

1.  Reduce human intervention.
2.  Reduce runtime, improve speed.
3.  Reduce Token usage, lower LLM costs.

### Design

```mermaid
graph TD

   subgraph Agent
   A_1[Protocol Spec]
   A_1 --> A_2[Protocol Code]
   A_1 --> A_A[Implementation Spec]
   A_1 --> A_B[Test Spec]
   A_1 --> A_C[Benchmark Spec]
   A_A --> A_A_1[Implementation Code]
   A_B --> A_B_1[Test Code]
   A_C --> A_C_1[Benchmark Code]
   A_A_1 --> A_D[Report]
   A_B_1 --> A_D
   A_C_1 --> A_D
   end

   subgraph Human
   H_1[Intention] -->|Dispatch| A_1
   A_D -->|Review| H_1
   end

```

1.  **Rapid Intention Alignment**

    The human aligns the module's functional requirements with the Agent through intention description, quickly outputting a Protocol Spec.

    The Protocol Spec includes the module's interface definitions, input/output data formats, functional descriptions, etc., essentially similar to an RFC document. Humans need to focus on interface definitions and functional descriptions to ensure clear module boundaries, especially evaluating the taste of the interface style.

    This process can be completed through multiple rounds of interaction. The Agent will continuously revise the Protocol Spec based on human feedback until approved by the human.

    Next, a lengthy automated implementation process follows, during which human intervention is not required. Two outcomes are possible: 1. The module implementation succeeds, generating a final report for human review; 2. The module implementation fails, generating an arbitration request for human intervention.

2.  **Generate Protocol Code from Protocol Spec**

    The Agent generates the skeleton code, Protocol Code, for the module based on the Protocol Spec, including interface definitions and comments.
    The Protocol Code will be used for subsequent implementation, test, and benchmark code generation. Its main purpose is to ensure clear module boundaries and avoid unnecessary complexity during implementation.

3.  **Parallel Generation of Implementation Spec, Test Spec, Benchmark Spec from Protocol Spec**

    Different Agents are tasked to generate the Implementation Spec, Test Spec, and Benchmark Spec based on the Protocol Spec, describing the module's implementation details, test cases, and benchmark testing plan, respectively.

4.  **Generate Test Code from Test Spec**

    A specialized testing Agent generates the module's unit test code, Test Code, based on the Protocol Spec and Test Spec, including various test cases and assertions. Interface-based testing methods must be used to avoid coupling with implementation details.

5.  **Generate Benchmark Code from Benchmark Spec**

    A specialized benchmark testing Agent generates the module's benchmark test code, Benchmark Code, based on the Protocol Spec and Benchmark Spec, including performance test cases and measurement metrics. Interface-based testing methods must be used to avoid coupling with implementation details.

6.  **Generate Implementation Code from Implementation Spec**

    A specialized implementation Agent generates the module's implementation code, Implementation Code, based on the Protocol Spec, Implementation Spec, Test Spec, and Benchmark Spec. Once implementation is complete, unit tests are run immediately.

    If unit tests fail, analyze the cause.

    -   If the issue is believed to be with the Implementation, modify the Implementation Spec and regenerate the Implementation Code. Repeat this process.
    -   If the issue is believed to be with the Test, collect details of the test failure and integrate them into a counter-argument. This will then be handled by a higher-level arbitration Agent.
        -   If the counter-argument is accepted, the arbitration Agent can choose to modify the Test Spec and rerun the tests. Repeat this process.
        -   If the counter-argument is rejected, the arbitration Agent generates an explanatory opinion, requiring the implementation Agent to modify the Implementation Spec and restart the implementation process. Repeat this process.
        -   **If the arbitration Agent deems it unable to judge, the arbitration Agent will request human intervention for arbitration.**

    If unit tests pass, proceed to benchmark testing.

7.  **Run Benchmark Tests**

    The Implementation Code that passed unit tests can run benchmark tests.

    If no other comparable implementation version exists, mark the current implementation as the baseline version, run the benchmark tests, record performance metrics, and pass the benchmark test.

    If other comparable implementation versions exist, run the benchmark tests and record performance metrics. Generate a comparison report for the Agent to analyze the performance changes of the current implementation version.

    -   If the current implementation version's performance degrades, analyze the cause.
        -   If the issue is believed to be with the Implementation, modify the Implementation Spec and regenerate the Implementation Code. Repeat this process.
        -   If the issue is believed to be with the Benchmark, collect details of the benchmark test failure and integrate them into a counter-argument. This will then be judged by a higher-level arbitration Agent.
            -   If the counter-argument is accepted, the arbitration Agent can choose to modify the Benchmark Spec and rerun the benchmark tests. Repeat this process. If the counter-argument is rejected, the arbitration Agent declares the task failed and generates a final report for human review.
            -   If the counter-argument is rejected, the arbitration Agent returns the counter-argument to the implementation Agent, requiring it to modify the Implementation Spec and restart the implementation process. Repeat this process.
            -   **If the arbitration Agent deems it unable to judge, the arbitration Agent will request human intervention for arbitration.**
    -   If the current implementation version's performance does not degrade, the benchmark test passes.

8.  **Generate Final Report**

    Once the Implementation Code passes both unit tests and benchmark tests, generate a final report containing implementation details, test results, and benchmark test results.
    The final report is submitted to the human for review. If the human approves the current implementation, the task is complete; otherwise, collect human feedback, integrate it into a counter-argument, and handle it via a higher-level arbitration Agent. If the counter-argument is accepted, the arbitration Agent can choose to modify the Protocol Spec and restart the entire implementation process. Repeat this process.

## Summary

1.  The core of the architecture is layered collaboration, specialized division of labor, and separation of concerns.
2.  A multi-level arbitration mechanism ensures implementation quality and reduces human intervention.
3.  Clear acceptance criteria (unit tests pass, performance does not degrade) establish a trust mechanism, eliminating the human desire for control.

Some unresolved issues remain:

1.  How to improve the quality of the Protocol Spec to ensure clear module boundaries? Add an automatic review step.
2.  How to avoid infinite arbitration loops? For example, set a maximum limit on automatic arbitration attempts.
3.  How to control actual execution time and Token usage within reasonable limits? Measure first, then optimize.
4.  How to guarantee the taste of interface design? For example, incorporate a team style guide.

Some prospects:

1.  Why must the human role be filled by a human? It is essentially a Supervisor. In the future, could higher-level AI replace humans for intention alignment and final review? This would further reduce human intervention and improve efficiency.
2.  Can this approach be extended beyond module-level tasks to larger-scale system design and implementation? For example, full-stack development tasks involving frontend, backend, and database? This would significantly enhance the application value of AI in the field of software engineering.