---
"title": "Discusión sobre la Observabilidad y los Métodos de Ingeniería del Código Generado por LLM"
"summary": "Este artículo documenta la discusión del autor con Hobo sobre la aplicación del código generado por LLM en entornos de producción. Los puntos clave incluyen: el código generado por LLM no puede usarse directamente en producción, debe garantizarse mediante pruebas rigurosas y observabilidad; la observabilidad requiere puntos de instrumentación intrusivos, aislamiento de recursos y sistemas de alerta, se sugiere incrustar las reglas de alerta en el código; el autor y Hobo tienen desacuerdos sobre la importancia de la inteligencia del LLM versus los métodos de ingeniería. El autor considera que los métodos de ingeniería (como las cadenas de prompts, los flujos de prueba) son más cruciales en la etapa actual, mientras que Hobo enfatiza el papel fundamental de la inteligencia del modelo. Ambas perspectivas son complementarias y valiosas para el equipo."
"tags":
  - "LLM"
  - "Observabilidad"
  - "Generación de Código"
  - "Métodos de Ingeniería"
  - "Inteligencia Artificial"
  - "Entorno de Producción"
  - "Pruebas"
"date": "2026-01-11"
---

Ahora es 11 de enero de 2026, domingo, madrugada.

Ayer almorcé con Hobo. Después de mucho tiempo sin vernos, hablamos de muchas cosas en la mesa. Él mostró mucho interés por nuestra situación reciente y nuestro trabajo. Intercambiamos muchas ideas.

Envidio que él, trabajando en una empresa extranjera, pueda usar sin límites LLMs como GPT y Claude Opus para asistir en su trabajo y mejorar la eficiencia. En contraste, en nuestro entorno laboral en China, todavía existen muchas restricciones e inconvenientes para usar estas herramientas.

Nuestro consenso es que, actualmente, en el trabajo de codificación, el código escrito por un LLM **no puede** ir directamente a un entorno de producción. Es muy, muy poco fiable.

### Observabilidad

Le pregunté: si nos limitamos a un módulo, y este pasa pruebas unitarias y pruebas de referencia (benchmarks) rigurosas, ¿podría usarse? Él añadió que también se necesita una **observabilidad muy buena**, ya que hay que considerar la estabilidad del servicio a largo plazo (long-run).
Además, el costo de dividir un sistema enorme en muchos módulos pequeños tan bien definidos es en sí mismo muy alto.

Este es efectivamente un problema que había pasado por alto antes. En [este artículo](czon://f14dd5dd9a733022055d249db9b1ed3d60d9b60b7eb8c063fe24c02774b6b631) mencioné que, después de pasar las pruebas de estilo de interfaz, las pruebas unitarias y las pruebas de referencia, uno puede empezar a confiar en el código generado por el LLM.
Una vez construí una prueba de referencia que consideraba el uso de CPU y memoria. Descubrí que, solo con pruebas de referencia ordinarias, no se pueden detectar los problemas de rendimiento en el código generado por el LLM. Es necesario realizar pruebas de estrés de antemano para descubrir los problemas.
Sin embargo, las pruebas de estrés tampoco pueden simular verdaderamente los diversos escenarios complejos de un entorno de producción. Por lo tanto, al final, se necesita una **observabilidad muy buena** para poder usar el código generado por el LLM en producción.

Pero, ¿cómo se deben diseñar y probar los sistemas de observabilidad?

La observabilidad en sí misma es también una herramienta para probar si la situación real cumple con las expectativas, solo que su entorno de ejecución es el entorno de producción, no el entorno de pruebas.

Además, puede necesitar ser **intrusiva** en el código de implementación para recopilar suficiente información. (La instrumentación intrusiva generalmente implica mayores costos de mantenimiento).

Si simplemente se recopilan algunos datos de métricas en el exterior de la interfaz y en la información del entorno, a menudo solo se pueden descubrir una parte de los problemas.
Por ejemplo, no se puede observar si el estado interno del módulo es correcto, si está consumiendo demasiados recursos, si tiene fugas de memoria, si hay interbloqueos (deadlocks), etc.

Y las métricas de observabilidad a menudo están relacionadas con el **aislamiento de recursos**, como CPU, memoria, E/S, etc. Sin un muy buen aislamiento de recursos, a menudo es difícil descubrir problemas.

Además, la clave de la observabilidad está en el **sistema de alertas**. Hobo mencionó una vez: "Cada punto de métrica (buried point) implica que debería tener una regla de alerta correspondiente; de lo contrario, ese punto de métrica no tiene sentido".

En la práctica habitual, las reglas de alerta son un trabajo de operaciones (Ops), pero la instrumentación (burying points) es un trabajo de desarrollo. Quizás esta práctica en sí misma sea problemática. ¿Por qué no consideramos incrustar directamente las reglas de alerta en el código?

Por ejemplo, cada punto de instrumentación podría llevar una definición de regla de alerta. Cuando una métrica supere un cierto umbral, se active automáticamente una alerta. De esta manera, los desarrolladores, al escribir el código, pueden considerar directamente la observabilidad y las reglas de alerta, mejorando así la calidad y fiabilidad del código.

Por ejemplo, al mismo tiempo que se instrumenta, se podría diseñar un mecanismo de aserción (assertion). Si la aserción no se cumple, se activa una alerta. Esto se parece mucho al mecanismo de error / warning. ¿Registrar un log de error / warning significa que necesita ser atendido?

Podríamos empezar por los logs, registrando especialmente los logs de error y warning, utilizando estos logs como parte de la observabilidad y combinándolos con el sistema de alertas para mejorar la fiabilidad del sistema.

Estoy muy de acuerdo con el punto de vista de Hobo: el código que va a producción **debe** tener una observabilidad muy buena; de lo contrario, no se puede garantizar la estabilidad a largo plazo (long-run).

### Nivel de Inteligencia del LLM vs. Métodos de Ingeniería

Además, Hobo también mencionó el problema de la influencia del propio nivel de inteligencia del LLM en la calidad del código. Aquí tenemos algunos desacuerdos.

Él cree que el nivel de inteligencia del LLM es el factor clave que determina la calidad del código. Si el nivel de inteligencia es insuficiente, no puede completar la tarea.
Yo creo que, aunque el nivel de inteligencia del LLM es importante, lo más crucial es cómo diseñar bien la tarea y el flujo de pruebas para asegurar que el código generado cumpla con las expectativas.

Hobo se inclina hacia la capacidad de élite, es un punto de vista **innatista**; yo me inclino más hacia la optimización del sistema, es un punto de vista **constructivista**.

Ambos tienen razón, pero en etapas diferentes.

-   **Por debajo del punto crítico de capacidad del modelo, yo tengo razón absoluta.** Para la gran mayoría de las aplicaciones comerciales actuales, el valor de los métodos de ingeniería es mucho mayor que esperar al siguiente modelo "más inteligente". Una cadena de prompts (prompt chain) cuidadosamente diseñada, un conjunto completo de pruebas (test suite) y un proceso iterativo pueden hacer que un modelo de capacidad media produzca código estable y utilizable. Este es el camino principal y exitoso para la implementación actual de aplicaciones de IA.

-   **Al enfrentar verdaderos límites cognitivos, el punto de vista de Hobo se hace evidente.** Cuando la complejidad de la tarea alcanza un nivel que requiere una verdadera comprensión, abstracción e innovación (por ejemplo, diseñar un algoritmo completamente nuevo, o entender un requisito extremadamente vago y contradictorio), el "techo cognitivo" del modelo se convierte en un obstáculo insuperable. En ese momento, ni el mejor proceso puede hacer que el modelo complete algo que "cognitivamente no puede hacer".

Yo represento la **"pragmática del ingeniero"**, que es la fuerza motriz central para que la IA cree valor en el presente.
Hobo representa la **"visión del investigador"**, que se centra en los puntos de ruptura de las capacidades futuras.

El estado ideal es **"inteligencia de nivel élite"** más **"métodos de ingeniería de nivel élite"**.

Usar el mejor proceso para estimular y dominar la inteligencia más poderosa. Nuestra divergencia no es sobre quién tiene razón o no, sino sobre el enfoque (optimización actual vs. avance fundamental) y la escala de tiempo (implementación a corto plazo vs. evolución a largo plazo).

En un equipo, esta perspectiva complementaria es muy valiosa.