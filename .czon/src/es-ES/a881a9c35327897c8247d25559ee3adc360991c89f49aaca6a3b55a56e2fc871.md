---
title: Arquitectura de Ingeniería de Software para Colaboración Humano-Máquina a Nivel de Módulo
summary: Este artículo explora las limitaciones de los LLM en la ingeniería de software colaborativa humano-máquina a nivel de módulo, como la coordinación no obligatoria y la potencia de cálculo limitada, y propone métodos para reducir la intervención humana mediante la nomenclatura de interfaces, pruebas unitarias y pruebas de referencia, con el objetivo de mejorar la eficiencia y reducir costos.
tags:
  - Colaboración Humano-Máquina
  - Ingeniería de Software
  - LLM
  - Agente de IA
  - Diseño de Módulos
  - Intervención Humana
  - Optimización de Eficiencia
inferred_lang: zh-Hans
---

# Arquitectura de Ingeniería de Software para Colaboración Humano-Máquina a Nivel de Módulo

## Contexto del Problema

Diseñar una arquitectura de ingeniería colaborativa humano-máquina a nivel de módulo utilizando LLM, con el objetivo de completar de manera eficiente el diseño, implementación e iteración de módulos de aplicaciones de nivel industrial, reduciendo los costos de intervención humana.

1.  Los Agentes de IA existentes (Claude Code, CodeX) tienen una calidad de implementación de módulos de código muy deficiente, aún requiriendo una alta intervención, reelaboración y revisión por parte de humanos.
2.  Los Agentes de IA existentes tienen dificultades para definir los límites de los módulos durante la implementación, lo que resulta en la escritura de código con mucha complejidad innecesaria.
3.  La implementación de los Agentes de IA existentes es demasiado lenta; una tarea desde su asignación hasta su aceptación requiere entre 10 y 30 minutos.

## Perspectiva del Problema

### ¿Por qué los Agentes no pueden completar todo el trabajo de una sola vez?

Porque la inteligencia de los LLM siempre estará limitada.

Hay dos razones principales: (Restricción Arquitectónica) Coordinación No Obligatoria y (Restricción de Recursos) Presupuesto de Potencia de Cálculo Limitada. Una razón secundaria es (Restricción Cognitiva) la Incompresibilidad Cognitiva.

**Coordinación No Obligatoria**: Se obliga al LLM a decir o hacer algo, pero no se puede garantizar que el resultado cumpla con todas las restricciones. Si se asume que el LLM debe responder y al mismo tiempo se le obliga a cumplir todas las restricciones, entonces cuando se plantea un problema difícil o incluso irresoluble, el cálculo colapsará. En su diseño subyacente, los LLM optan por una coordinación no obligatoria, eligiendo en su lugar generar el Token con mayor probabilidad. (Problema de la Parada)
Algunos problemas complejos requieren encontrar una buena solución entre múltiples puntos de interés en conflicto. El Contexto del LLM no puede contener simultáneamente la gestión de proyectos, el diseño, la implementación y la aceptación, ya que son completamente conflictivos y antagónicos. Solo una arquitectura con separación de intereses puede evitar que el Agente se atasque. Antes de generar una salida, el LLM no está realmente obligado a cumplir todas las restricciones, porque eso causaría un colapso en su razonamiento.

**Presupuesto de Potencia de Cálculo Limitada**: Es evidente que un LLM más inteligente tiene la capacidad de considerar más puntos de interés, pero el costo será inevitablemente un mayor consumo de potencia de cálculo. El llamado AGI es siempre un objetivo máximo bajo condiciones de costo ilimitado, no el objetivo de un LLM comercial y de propósito general. Solo el negocio puede decidir si utilizar tanta potencia de cálculo para resolver un problema, por lo que un LLM de propósito general no puede tomar esta decisión por sí mismo. Esta contradicción fundamental no tiene solución. (Restricción Económica)
Nótese el Trilema de Münchhausen: la infinitud del pensamiento mismo está en conflicto perpetuo con la finitud del presupuesto de razonamiento. Idealmente, deseamos que la IA sea una especie de entidad divina, pero en realidad siempre será una inteligencia limitada, simplemente porque su pensamiento consume recursos.

Razón relativamente secundaria:

**Incompresibilidad Cognitiva**: La lógica del negocio debe recibir suficiente información de entrada para ser resuelta. Si creemos que una frase puede explicarle claramente algo al LLM, esto implica que la transmisión de ese conocimiento es compresible. En realidad, es necesario complementar con mucho contexto para que el LLM comprenda la complejidad del entorno en el que se encuentra.
¿Por qué se considera una razón secundaria? Este motivo presenta algunos desafíos: en primer lugar, el conocimiento común ya está incorporado en el preentrenamiento del LLM, y muchos aspectos cognitivos se convertirán gradualmente en problemas menores a medida que los LLM sean más potentes. En segundo lugar, el LLM puede adivinar y completar rápidamente, alineándose con el ser humano, lo que también mitiga la contradicción. Sin embargo, basándonos en la situación actual, sigue siendo un problema considerable.
Creemos que muchos usuarios experimentados de LLM pueden reconocer los problemas que esto conlleva.

Sin embargo, una buena noticia es que el umbral de emergencia de los Agentes es que al menos puedan generar una salida efectiva para un punto de interés, y esto claramente ya se ha desbloqueado. Necesitamos aprovechar a largo plazo esta inteligencia limitada como una herramienta para completar nuestras tareas de negocio.

### ¿Cómo reducir la intervención humana?

La clave está en disipar las preocupaciones humanas; entonces, las personas, adoptando una mentalidad de "si funciona, ya está bien", dejarán de exigir más del trabajo producido por la IA.

Entonces, ¿qué verificaciones, una vez superadas, harán que una persona juzgue que ya no tiene capacidad para intervenir o que no es necesario tomar más medidas?

1.  La nomenclatura conceptual de las interfaces externas del módulo cumple con los requisitos y tiene buen gusto. Esto disipa la preocupación de que interfaces irrazonables se propaguen a las partes posteriores del sistema.
2.  Pasa las pruebas unitarias. Esto disipa la preocupación sobre si el módulo funcionará correctamente.
3.  Muestra optimización o no empeora en las pruebas de referencia. Esto disipa la preocupación sobre si el módulo es muy ineficiente.
    El primer punto se puede detectar en la fase inicial, mientras que los otros dos solo se conocen al finalizar los experimentos. Si se cumplen los tres, los humanos no tienen motivos para forzar la intervención en el trabajo completado por la IA.

En cuanto a si este módulo puede realmente manejar los patrones de datos reales, es necesario probarlo con datos del entorno de producción. Luego, una persona resume sus patrones y, a través de la intención, construye un nuevo módulo para resolver el nuevo problema. Este tema queda fuera del alcance de este artículo por el momento.

### Objetivos Prioritarios

1.  Reducir la intervención humana.
2.  Reducir el tiempo de ejecución, mejorar la velocidad.
3.  Reducir el consumo de Tokens, disminuir los costos del LLM.