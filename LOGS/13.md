2026 年 1 月 13 日，周二，早上。

今天也是早起的一天，7 点多就醒了。醒来发现 DeepSeek 发了新论文。提出了一种新的技术 Engram。

[DeepSeek - Engram 开源地址](https://github.com/deepseek-ai/Engram)，包含了 Demo 和论文 PDF。

论文标题是 _Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models._

论文的核心思想是引入了一种新的记忆机制，允许模型在生成文本时动态地查询和利用外部存储的记忆片段，从而提升模型的上下文理解和生成能力。

这种机制通过一个可扩展的查找表实现，允许模型在需要时访问相关的记忆内容，而不是仅依赖于模型内部的参数。这种方法不仅提高了模型的性能，还显著降低了计算资源的消耗，使得大规模语言模型在资源受限的环境中也能高效运行。

这种记忆机制的引入，为大语言模型的发展开辟了新的方向，特别是在处理长文本和复杂任务时，能够更好地利用外部知识和上下文信息。

此外，论文还对比了 Engram 与 MoE 的成分占比的最优化问题，发现 Engram / MoE 的比例，对性能的影响会呈现 U 型曲线。这说明在设计大模型时，如何平衡不同组件的比例，是一个需要仔细考虑的问题。

哲学上来说，从 Attention is All You Need，到 Mixture of Experts，再到现在的 Engram，都是在探索如何更高效地利用模型的参数和计算资源，以提升模型的表达能力和泛化能力。从干细胞到分化细胞，再到器官系统，每一步都是对复杂系统如何高效运作的探索。未来我们或许会看到更多类似的创新，推动大语言模型向更智能、更高效的方向发展。

总的来说，这篇论文为大语言模型的记忆机制提供了新的思路，值得进一步研究和探索。

值得关注的是，将要发布的 DeepSeek v4 会给人带来怎样的惊喜？

期待 ing...
